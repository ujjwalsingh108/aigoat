# ğŸ“Š Technical Chart Pattern Detection System
**Architecture for Low-Latency Pattern Analysis at Scale**

---

## 1. SYSTEM ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          BREAKOUT SCANNER (Main Loop)                     â”‚
â”‚                         Runs every 15min @ market hours                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Stage 1: RULE FILTER    â”‚
                    â”‚   (Existing 6 Criteria)   â”‚
                    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
                    â”‚   â€¢ Momentum               â”‚
                    â”‚   â€¢ Volume surge           â”‚
                    â”‚   â€¢ Trend alignment        â”‚
                    â”‚   â€¢ Volatility             â”‚
                    â”‚   â€¢ Support/Resistance     â”‚
                    â”‚   â€¢ Price action           â”‚
                    â”‚                            â”‚
                    â”‚   Output: ~100-300 stocks  â”‚
                    â”‚   (from 2500 universe)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Stage 2: PATTERN ENGINE (DETERMINISTIC)         â”‚
                    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
                    â”‚   Pre-computed indicators (cached):               â”‚
                    â”‚   â€¢ Rolling highs/lows (5,10,20,50 bars)          â”‚
                    â”‚   â€¢ ATR(14)                                        â”‚
                    â”‚   â€¢ Volume MA(20)                                  â”‚
                    â”‚   â€¢ Price swing points                             â”‚
                    â”‚   â€¢ Trendlines (vectorized)                        â”‚
                    â”‚                                                    â”‚
                    â”‚   Pattern Detection (parallel):                   â”‚
                    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚   â”‚Candlestickâ”‚ Trianglesâ”‚ Reversalsâ”‚Continuationâ”‚   â”‚
                    â”‚   â”‚  (~2ms)  â”‚  (~5ms)  â”‚ (~10ms)  â”‚  (~5ms)  â”‚   â”‚
                    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚                                                    â”‚
                    â”‚   Confidence Scorer:                               â”‚
                    â”‚   â€¢ Geometric validation                           â”‚
                    â”‚   â€¢ Volume confirmation                            â”‚
                    â”‚   â€¢ Trend context                                  â”‚
                    â”‚   â€¢ Breakout strength                              â”‚
                    â”‚                                                    â”‚
                    â”‚   Output: {symbol, patterns[], scores, metrics}   â”‚
                    â”‚   Latency: <50ms per stock, <150ms total          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   WRITE TO DATABASE (bullish/bearish signals)     â”‚
                    â”‚   With pattern metadata for UI rendering          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   FRONTEND DASHBOARD (React)                      â”‚
                    â”‚   â€¢ Shows filtered stocks                          â”‚
                    â”‚   â€¢ Displays matched patterns                      â”‚
                    â”‚   â€¢ Pattern confidence badge                       â”‚
                    â”‚   â€¢ "AI Validate" button (per stock)              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                          User clicks "AI Validate"
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Stage 3: AI VALIDATION (ON-DEMAND)              â”‚
                    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
                    â”‚   API Route: POST /api/validate-breakout          â”‚
                    â”‚                                                    â”‚
                    â”‚   Input Preparation:                               â”‚
                    â”‚   {                                                â”‚
                    â”‚     symbol: "RELIANCE",                            â”‚
                    â”‚     direction: "bullish",                          â”‚
                    â”‚     criteria: {                                    â”‚
                    â”‚       momentum_score: 0.85,                        â”‚
                    â”‚       volume_ratio: 2.3,                           â”‚
                    â”‚       trend_strength: 0.72,                        â”‚
                    â”‚       volatility_percentile: 68,                   â”‚
                    â”‚       support_distance: 0.5%,                      â”‚
                    â”‚       price_action_score: 0.91                     â”‚
                    â”‚     },                                             â”‚
                    â”‚     patterns: [                                    â”‚
                    â”‚       {name: "BULL_FLAG", confidence: 87},         â”‚
                    â”‚       {name: "ASCENDING_TRIANGLE", confidence: 75} â”‚
                    â”‚     ],                                             â”‚
                    â”‚     price_context: {                               â”‚
                    â”‚       current: 2850,                               â”‚
                    â”‚       day_change: +1.8%,                           â”‚
                    â”‚       week_high: 2860,                             â”‚
                    â”‚       volume_surge: 2.3x,                          â”‚
                    â”‚       breakout_level: 2820                         â”‚
                    â”‚     }                                              â”‚
                    â”‚   }                                                â”‚
                    â”‚                                                    â”‚
                    â”‚   LLM Call (Groq API):                             â”‚
                    â”‚   â€¢ Model: llama-3.3-70b-versatile                 â”‚
                    â”‚   â€¢ Temperature: 0.1 (deterministic)               â”‚
                    â”‚   â€¢ Max tokens: 200                                â”‚
                    â”‚   â€¢ Latency: ~500ms                                â”‚
                    â”‚                                                    â”‚
                    â”‚   Output:                                          â”‚
                    â”‚   {                                                â”‚
                    â”‚     verdict: "TRUE_POSITIVE",                      â”‚
                    â”‚     confidence: 0.88,                              â”‚
                    â”‚     reasoning: "Bull flag with volume confirmation,â”‚
                    â”‚                 strong momentum alignment. Entry   â”‚
                    â”‚                 above 2820 validated.",            â”‚
                    â”‚     risk_factors: ["Watch for gap fill at 2780"]  â”‚
                    â”‚   }                                                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. PATTERN DETECTION ALGORITHMS

### 2.1 CANDLESTICK PATTERNS (Lookback: 1-3 bars)

```python
class CandlestickDetector:
    """
    Vectorized candlestick pattern detection
    Input: OHLCV arrays [N_stocks x N_bars]
    Output: Pattern flags [N_stocks x N_patterns]
    """
    
    def __init__(self, ohlc: np.ndarray, atr: np.ndarray):
        self.o = ohlc[:, :, 0]  # Open
        self.h = ohlc[:, :, 1]  # High
        self.l = ohlc[:, :, 2]  # Low
        self.c = ohlc[:, :, 3]  # Close
        self.v = ohlc[:, :, 4]  # Volume
        self.atr = atr
        
        # Pre-compute common metrics
        self.body = self.c - self.o
        self.body_abs = np.abs(self.body)
        self.range = self.h - self.l
        self.upper_shadow = self.h - np.maximum(self.o, self.c)
        self.lower_shadow = np.minimum(self.o, self.c) - self.l
        self.body_ratio = self.body_abs / (self.range + 1e-8)
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DOJI (Indecision)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_doji(self) -> np.ndarray:
        """
        Conditions:
        - Body < 10% of range
        - Upper/lower shadows roughly equal (Â±30%)
        - Range > 0.5 * ATR (not a flat candle)
        """
        is_small_body = self.body_ratio[:, -1] < 0.10
        shadow_balance = np.abs(
            self.upper_shadow[:, -1] - self.lower_shadow[:, -1]
        ) < 0.30 * self.range[:, -1]
        sufficient_range = self.range[:, -1] > 0.5 * self.atr[:, -1]
        
        return is_small_body & shadow_balance & sufficient_range
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HAMMER (Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_hammer(self) -> np.ndarray:
        """
        Conditions:
        - Small body at top (bullish or bearish)
        - Lower shadow â‰¥ 2x body
        - Upper shadow < 0.2x body
        - In downtrend (price < MA20)
        - Volume > MA(20)
        """
        small_body = self.body_abs[:, -1] < 0.5 * self.range[:, -1]
        long_lower_shadow = (
            self.lower_shadow[:, -1] >= 2.0 * self.body_abs[:, -1]
        )
        short_upper_shadow = (
            self.upper_shadow[:, -1] <= 0.2 * self.body_abs[:, -1]
        )
        in_downtrend = self.c[:, -1] < self.c[:, -20:].mean(axis=1)
        volume_confirm = self.v[:, -1] > self.v[:, -20:].mean(axis=1)
        
        return (
            small_body & long_lower_shadow & short_upper_shadow 
            & in_downtrend & volume_confirm
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HANGING MAN (Bearish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_hanging_man(self) -> np.ndarray:
        """Same as hammer BUT in uptrend"""
        small_body = self.body_abs[:, -1] < 0.5 * self.range[:, -1]
        long_lower_shadow = (
            self.lower_shadow[:, -1] >= 2.0 * self.body_abs[:, -1]
        )
        short_upper_shadow = (
            self.upper_shadow[:, -1] <= 0.2 * self.body_abs[:, -1]
        )
        in_uptrend = self.c[:, -1] > self.c[:, -20:].mean(axis=1)
        volume_confirm = self.v[:, -1] > self.v[:, -20:].mean(axis=1)
        
        return (
            small_body & long_lower_shadow & short_upper_shadow 
            & in_uptrend & volume_confirm
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SHOOTING STAR (Bearish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_shooting_star(self) -> np.ndarray:
        """
        Inverse of hammer - long upper shadow, small body at bottom
        """
        small_body = self.body_abs[:, -1] < 0.5 * self.range[:, -1]
        long_upper_shadow = (
            self.upper_shadow[:, -1] >= 2.0 * self.body_abs[:, -1]
        )
        short_lower_shadow = (
            self.lower_shadow[:, -1] <= 0.2 * self.body_abs[:, -1]
        )
        in_uptrend = self.c[:, -1] > self.c[:, -20:].mean(axis=1)
        
        return (
            small_body & long_upper_shadow & short_lower_shadow & in_uptrend
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INVERTED HAMMER (Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_inverted_hammer(self) -> np.ndarray:
        """Same as shooting star BUT in downtrend + confirmation needed"""
        small_body = self.body_abs[:, -1] < 0.5 * self.range[:, -1]
        long_upper_shadow = (
            self.upper_shadow[:, -1] >= 2.0 * self.body_abs[:, -1]
        )
        short_lower_shadow = (
            self.lower_shadow[:, -1] <= 0.2 * self.body_abs[:, -1]
        )
        in_downtrend = self.c[:, -1] < self.c[:, -20:].mean(axis=1)
        
        return (
            small_body & long_upper_shadow & short_lower_shadow & in_downtrend
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BULLISH ENGULFING (Strong Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bullish_engulfing(self) -> np.ndarray:
        """
        Conditions:
        - Candle[-2] is bearish (red)
        - Candle[-1] is bullish (green)
        - C[-1] > O[-2] (engulfs previous body)
        - O[-1] < C[-2]
        - Volume[-1] > Volume[-2] * 1.2
        - In downtrend or at support
        """
        prev_bearish = self.body[:, -2] < 0
        curr_bullish = self.body[:, -1] > 0
        engulfs_top = self.c[:, -1] > self.o[:, -2]
        engulfs_bottom = self.o[:, -1] < self.c[:, -2]
        volume_surge = self.v[:, -1] > self.v[:, -2] * 1.2
        in_downtrend = self.c[:, -2] < self.c[:, -22:-2].mean(axis=1)
        
        return (
            prev_bearish & curr_bullish & engulfs_top 
            & engulfs_bottom & volume_surge & in_downtrend
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BEARISH ENGULFING (Strong Bearish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bearish_engulfing(self) -> np.ndarray:
        """Mirror of bullish engulfing"""
        prev_bullish = self.body[:, -2] > 0
        curr_bearish = self.body[:, -1] < 0
        engulfs_bottom = self.c[:, -1] < self.o[:, -2]
        engulfs_top = self.o[:, -1] > self.c[:, -2]
        volume_surge = self.v[:, -1] > self.v[:, -2] * 1.2
        in_uptrend = self.c[:, -2] > self.c[:, -22:-2].mean(axis=1)
        
        return (
            prev_bullish & curr_bearish & engulfs_bottom 
            & engulfs_top & volume_surge & in_uptrend
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BULLISH HARAMI (Bullish Continuation/Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bullish_harami(self) -> np.ndarray:
        """
        Small bullish candle inside large bearish candle
        """
        prev_bearish = self.body[:, -2] < 0
        prev_large = self.body_abs[:, -2] > 0.7 * self.range[:, -2]
        curr_bullish = self.body[:, -1] > 0
        curr_small = self.body_abs[:, -1] < 0.4 * self.body_abs[:, -2]
        inside_body = (
            (self.o[:, -1] > self.c[:, -2]) & 
            (self.c[:, -1] < self.o[:, -2])
        )
        
        return prev_bearish & prev_large & curr_bullish & curr_small & inside_body
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BEARISH HARAMI (Bearish Continuation/Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bearish_harami(self) -> np.ndarray:
        """Mirror of bullish harami"""
        prev_bullish = self.body[:, -2] > 0
        prev_large = self.body_abs[:, -2] > 0.7 * self.range[:, -2]
        curr_bearish = self.body[:, -1] < 0
        curr_small = self.body_abs[:, -1] < 0.4 * self.body_abs[:, -2]
        inside_body = (
            (self.o[:, -1] < self.c[:, -2]) & 
            (self.c[:, -1] > self.o[:, -2])
        )
        
        return prev_bullish & prev_large & curr_bearish & curr_small & inside_body
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DARK CLOUD COVER (Bearish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_dark_cloud_cover(self) -> np.ndarray:
        """
        - Candle[-2] is strong bullish
        - Candle[-1] opens above C[-2], closes below 50% of body[-2]
        - In uptrend
        """
        prev_bullish = self.body[:, -2] > 0.5 * self.range[:, -2]
        curr_bearish = self.body[:, -1] < 0
        opens_above = self.o[:, -1] > self.c[:, -2]
        closes_below_midpoint = (
            self.c[:, -1] < (self.o[:, -2] + self.c[:, -2]) / 2
        )
        in_uptrend = self.c[:, -3:-1].mean(axis=1) > self.c[:, -23:-3].mean(axis=1)
        
        return (
            prev_bullish & curr_bearish & opens_above 
            & closes_below_midpoint & in_uptrend
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PIERCING PATTERN (Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_piercing_pattern(self) -> np.ndarray:
        """Mirror of dark cloud cover"""
        prev_bearish = self.body[:, -2] < -0.5 * self.range[:, -2]
        curr_bullish = self.body[:, -1] > 0
        opens_below = self.o[:, -1] < self.c[:, -2]
        closes_above_midpoint = (
            self.c[:, -1] > (self.o[:, -2] + self.c[:, -2]) / 2
        )
        in_downtrend = self.c[:, -3:-1].mean(axis=1) < self.c[:, -23:-3].mean(axis=1)
        
        return (
            prev_bearish & curr_bullish & opens_below 
            & closes_above_midpoint & in_downtrend
        )
```

### 2.2 TRIANGLE PATTERNS (Lookback: 15-50 bars)

```python
class TriangleDetector:
    """
    Detect converging trendlines using swing highs/lows
    Requires pre-computed pivot points
    """
    
    def __init__(self, highs, lows, pivots_high, pivots_low):
        self.h = highs
        self.l = lows
        self.ph = pivots_high  # Boolean array marking swing highs
        self.pl = pivots_low   # Boolean array marking swing lows
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ASCENDING TRIANGLE (Bullish Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_ascending_triangle(self, lookback=30) -> dict:
        """
        Conditions:
        - Resistance: â‰¥2 swing highs at similar level (Â±1%)
        - Support: Rising trendline from â‰¥2 swing lows
        - Slope(resistance) â‰ˆ 0Â°
        - Slope(support) > 0Â° (rising)
        - Convergence within 15-40 bars
        - Volume: Declining into apex, surge on breakout
        """
        results = []
        
        for i in range(len(self.h)):
            # Get last N swing highs
            swing_highs = self.h[i, -lookback:][self.ph[i, -lookback:]]
            swing_lows = self.l[i, -lookback:][self.pl[i, -lookback:]]
            
            if len(swing_highs) < 2 or len(swing_lows) < 2:
                continue
            
            # Check resistance flatness (coefficient of variation < 1%)
            resistance_level = swing_highs[-3:].mean()
            resistance_std = swing_highs[-3:].std()
            is_flat_resistance = (resistance_std / resistance_level) < 0.01
            
            # Check support slope (linear regression)
            support_slope = np.polyfit(
                range(len(swing_lows[-3:])), swing_lows[-3:], 1
            )[0]
            is_rising_support = support_slope > 0
            
            # Check convergence (distance narrowing)
            initial_gap = resistance_level - swing_lows[-3]
            current_gap = resistance_level - swing_lows[-1]
            is_converging = current_gap < initial_gap * 0.7
            
            if is_flat_resistance and is_rising_support and is_converging:
                # Calculate breakout level
                breakout_price = resistance_level * 1.005  # 0.5% above resistance
                target_distance = initial_gap
                
                results.append({
                    'pattern': 'ASCENDING_TRIANGLE',
                    'direction': 'bullish',
                    'resistance': resistance_level,
                    'support_slope': support_slope,
                    'breakout_level': breakout_price,
                    'target': breakout_price + target_distance,
                    'confidence': 75
                })
        
        return results
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DESCENDING TRIANGLE (Bearish Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_descending_triangle(self, lookback=30) -> dict:
        """Mirror of ascending triangle - flat support, descending resistance"""
        results = []
        
        for i in range(len(self.h)):
            swing_highs = self.h[i, -lookback:][self.ph[i, -lookback:]]
            swing_lows = self.l[i, -lookback:][self.pl[i, -lookback:]]
            
            if len(swing_highs) < 2 or len(swing_lows) < 2:
                continue
            
            # Flat support
            support_level = swing_lows[-3:].mean()
            support_std = swing_lows[-3:].std()
            is_flat_support = (support_std / support_level) < 0.01
            
            # Descending resistance
            resistance_slope = np.polyfit(
                range(len(swing_highs[-3:])), swing_highs[-3:], 1
            )[0]
            is_falling_resistance = resistance_slope < 0
            
            # Convergence
            initial_gap = swing_highs[-3] - support_level
            current_gap = swing_highs[-1] - support_level
            is_converging = current_gap < initial_gap * 0.7
            
            if is_flat_support and is_falling_resistance and is_converging:
                breakout_price = support_level * 0.995  # 0.5% below support
                target_distance = initial_gap
                
                results.append({
                    'pattern': 'DESCENDING_TRIANGLE',
                    'direction': 'bearish',
                    'support': support_level,
                    'resistance_slope': resistance_slope,
                    'breakout_level': breakout_price,
                    'target': breakout_price - target_distance,
                    'confidence': 75
                })
        
        return results
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SYMMETRICAL TRIANGLE (Neutral - Direction = Trend)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_symmetrical_triangle(self, lookback=30) -> dict:
        """Both trendlines converging - follow prevailing trend"""
        results = []
        
        for i in range(len(self.h)):
            swing_highs = self.h[i, -lookback:][self.ph[i, -lookback:]]
            swing_lows = self.l[i, -lookback:][self.pl[i, -lookback:]]
            
            if len(swing_highs) < 3 or len(swing_lows) < 3:
                continue
            
            # Both slopes converging
            resistance_slope = np.polyfit(range(len(swing_highs[-3:])), swing_highs[-3:], 1)[0]
            support_slope = np.polyfit(range(len(swing_lows[-3:])), swing_lows[-3:], 1)[0]
            
            is_converging = (
                resistance_slope < 0 and  # Falling resistance
                support_slope > 0 and     # Rising support
                abs(resistance_slope) > 0.1 and  # Significant slopes
                abs(support_slope) > 0.1
            )
            
            if is_converging:
                # Determine direction from pre-triangle trend
                pre_trend_slope = np.polyfit(range(lookback), self.h[i, -lookback:], 1)[0]
                direction = 'bullish' if pre_trend_slope > 0 else 'bearish'
                
                results.append({
                    'pattern': 'SYMMETRICAL_TRIANGLE',
                    'direction': direction,
                    'confidence': 65,  # Lower confidence (50/50 break)
                    'note': 'Wait for confirmed breakout'
                })
        
        return results
```

### 2.3 REVERSAL PATTERNS (Lookback: 30-100 bars)

```python
class ReversalDetector:
    """
    Complex multi-swing reversal patterns
    Requires robust peak/trough detection
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HEAD AND SHOULDERS (Bearish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_head_and_shoulders(self, highs, lows, lookback=60) -> dict:
        """
        Geometry:
        - 3 peaks: left_shoulder < head > right_shoulder
        - Shoulders at similar height (Â±5%)
        - Neckline: Support connecting troughs between peaks
        - Symmetry: Time between peaks roughly equal (Â±30%)
        - Volume: Declining on right shoulder
        
        Vectorized approach:
        1. Find all peaks using scipy.signal.find_peaks()
        2. Filter for 3-peak sequences
        3. Validate geometry constraints
        """
        peaks_idx = find_peaks(highs, distance=5)[0]
        
        if len(peaks_idx) < 3:
            return None
        
        # Check last 3 peaks
        ls_idx, head_idx, rs_idx = peaks_idx[-3:]
        ls_price = highs[ls_idx]
        head_price = highs[head_idx]
        rs_price = highs[rs_idx]
        
        # Validate geometry
        is_head_highest = (head_price > ls_price) and (head_price > rs_price)
        shoulders_match = abs(ls_price - rs_price) / ls_price < 0.05
        
        if not (is_head_highest and shoulders_match):
            return None
        
        # Find neckline (lows between peaks)
        trough1_idx = ls_idx + np.argmin(lows[ls_idx:head_idx])
        trough2_idx = head_idx + np.argmin(lows[head_idx:rs_idx])
        neckline_level = max(lows[trough1_idx], lows[trough2_idx])
        
        # Breakout target = Head height
        target_distance = head_price - neckline_level
        
        return {
            'pattern': 'HEAD_AND_SHOULDERS',
            'direction': 'bearish',
            'head_level': head_price,
            'neckline': neckline_level,
            'breakout_level': neckline_level * 0.995,
            'target': neckline_level - target_distance,
            'confidence': 80,
            'symmetry_score': 1 - abs(ls_price - rs_price) / ls_price
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INVERSE HEAD AND SHOULDERS (Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_inverse_head_and_shoulders(self, highs, lows, lookback=60) -> dict:
        """Mirror of H&S - 3 troughs with middle lowest"""
        troughs_idx = find_peaks(-lows, distance=5)[0]
        
        if len(troughs_idx) < 3:
            return None
        
        ls_idx, head_idx, rs_idx = troughs_idx[-3:]
        ls_price = lows[ls_idx]
        head_price = lows[head_idx]
        rs_price = lows[rs_idx]
        
        is_head_lowest = (head_price < ls_price) and (head_price < rs_price)
        shoulders_match = abs(ls_price - rs_price) / ls_price < 0.05
        
        if not (is_head_lowest and shoulders_match):
            return None
        
        # Neckline from peaks between troughs
        peak1_idx = ls_idx + np.argmax(highs[ls_idx:head_idx])
        peak2_idx = head_idx + np.argmax(highs[head_idx:rs_idx])
        neckline_level = min(highs[peak1_idx], highs[peak2_idx])
        
        target_distance = neckline_level - head_price
        
        return {
            'pattern': 'INVERSE_HEAD_AND_SHOULDERS',
            'direction': 'bullish',
            'head_level': head_price,
            'neckline': neckline_level,
            'breakout_level': neckline_level * 1.005,
            'target': neckline_level + target_distance,
            'confidence': 80
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DOUBLE TOP (Bearish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_double_top(self, highs, lows, lookback=40) -> dict:
        """
        - 2 peaks at similar level (Â±2%)
        - Trough between peaks â‰¥5% below peaks
        - Time between peaks: 10-40 bars
        - Volume: Lower on 2nd peak
        """
        peaks_idx = find_peaks(highs, distance=10)[0]
        
        if len(peaks_idx) < 2:
            return None
        
        peak1_idx, peak2_idx = peaks_idx[-2:]
        peak1 = highs[peak1_idx]
        peak2 = highs[peak2_idx]
        
        # Peaks at same level
        peak_similarity = abs(peak1 - peak2) / peak1
        if peak_similarity > 0.02:  # More than 2% difference
            return None
        
        # Significant trough
        trough_idx = peak1_idx + np.argmin(lows[peak1_idx:peak2_idx])
        trough = lows[trough_idx]
        trough_depth = (peak1 - trough) / peak1
        
        if trough_depth < 0.05:  # Less than 5% pullback
            return None
        
        neckline = trough
        target_distance = peak1 - neckline
        
        return {
            'pattern': 'DOUBLE_TOP',
            'direction': 'bearish',
            'top_level': (peak1 + peak2) / 2,
            'neckline': neckline,
            'breakout_level': neckline * 0.995,
            'target': neckline - target_distance,
            'confidence': 75
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DOUBLE BOTTOM (Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_double_bottom(self, highs, lows, lookback=40) -> dict:
        """Mirror of double top"""
        troughs_idx = find_peaks(-lows, distance=10)[0]
        
        if len(troughs_idx) < 2:
            return None
        
        trough1_idx, trough2_idx = troughs_idx[-2:]
        trough1 = lows[trough1_idx]
        trough2 = lows[trough2_idx]
        
        trough_similarity = abs(trough1 - trough2) / trough1
        if trough_similarity > 0.02:
            return None
        
        peak_idx = trough1_idx + np.argmax(highs[trough1_idx:trough2_idx])
        peak = highs[peak_idx]
        peak_height = (peak - trough1) / trough1
        
        if peak_height < 0.05:
            return None
        
        neckline = peak
        target_distance = neckline - trough1
        
        return {
            'pattern': 'DOUBLE_BOTTOM',
            'direction': 'bullish',
            'bottom_level': (trough1 + trough2) / 2,
            'neckline': neckline,
            'breakout_level': neckline * 1.005,
            'target': neckline + target_distance,
            'confidence': 75
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CUP AND HANDLE (Bullish Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_cup_and_handle(self, highs, lows, closes, lookback=60) -> dict:
        """
        - Cup: U-shaped bottom (30-50 bars)
        - Depth: 12-33% from prior high
        - Handle: 1-4 week consolidation with slight downward drift
        - Handle depth: <50% of cup depth
        - Volume: Declining in handle, surge on breakout
        
        Validation:
        - Fit parabola to cup section
        - Check RÂ² > 0.85 for U-shape
        """
        # Split into cup and handle
        cup_end = int(lookback * 0.75)
        cup_prices = closes[-lookback:-lookback+cup_end]
        handle_prices = closes[-lookback+cup_end:]
        
        # Check cup U-shape using polynomial fit
        x = np.arange(len(cup_prices))
        coeffs = np.polyfit(x, cup_prices, 2)
        fitted = np.polyval(coeffs, x)
        r_squared = 1 - (np.sum((cup_prices - fitted)**2) / 
                        np.sum((cup_prices - cup_prices.mean())**2))
        
        is_u_shaped = r_squared > 0.85 and coeffs[0] > 0  # Positive curvature
        
        if not is_u_shaped:
            return None
        
        # Validate cup depth
        prior_high = highs[-lookback]
        cup_low = cup_prices.min()
        cup_depth = (prior_high - cup_low) / prior_high
        
        if not (0.12 <= cup_depth <= 0.33):
            return None
        
        # Validate handle
        handle_high = handle_prices.max()
        handle_low = handle_prices.min()
        handle_depth = (handle_high - handle_low) / handle_high
        handle_is_shallow = handle_depth < 0.5 * cup_depth
        handle_drifts_down = handle_prices[-1] < handle_prices[0]
        
        if not (handle_is_shallow and handle_drifts_down):
            return None
        
        # Breakout level = Prior high
        breakout_level = prior_high * 1.005
        target_distance = prior_high - cup_low
        
        return {
            'pattern': 'CUP_AND_HANDLE',
            'direction': 'bullish',
            'prior_high': prior_high,
            'cup_low': cup_low,
            'breakout_level': breakout_level,
            'target': breakout_level + target_distance,
            'confidence': 85,
            'u_shape_r2': r_squared
        }
```

### 2.4 CONTINUATION PATTERNS (Lookback: 10-30 bars)

```python
class ContinuationDetector:
    """
    Flag, pennant, and wedge patterns
    Indicate trend continuation after brief consolidation
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BULL FLAG (Bullish Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bull_flag(self, highs, lows, closes, volumes, lookback=20) -> dict:
        """
        Geometry:
        - Flagpole: Sharp uptrend (â‰¥10% in 5-10 bars)
        - Flag: Parallel channel with slight downward drift (5-15 bars)
        - Channel angle: -10Â° to -45Â° (retraces 20-50% of pole)
        - Volume: Declining in flag, surge on breakout
        - Time: Flag should be â‰¤30% duration of pole
        
        Rejection rules:
        - Flag angle > 0Â° (upward) = Not a flag
        - Retracement > 60% = Reversal, not continuation
        - Flag duration > pole duration = Exhaustion
        """
        # Detect flagpole (last strong surge)
        pole_start = -30
        pole_end = -15
        pole_gain = (closes[pole_end] - closes[pole_start]) / closes[pole_start]
        
        if pole_gain < 0.10:  # Less than 10% gain
            return None
        
        # Detect flag (consolidation after pole)
        flag_prices = closes[-15:]
        flag_highs = highs[-15:]
        flag_lows = lows[-15:]
        
        # Check downward drift using linear regression
        x = np.arange(len(flag_prices))
        slope, intercept = np.polyfit(x, flag_prices, 1)
        
        # Flag should drift down but stay parallel
        angle_deg = np.degrees(np.arctan(slope / closes[-15]))
        is_downward = -45 <= angle_deg <= -10
        
        if not is_downward:
            return None
        
        # Check retracement
        pole_high = closes[pole_end]
        flag_low = flag_lows.min()
        retracement = (pole_high - flag_low) / (closes[pole_end] - closes[pole_start])
        
        if not (0.20 <= retracement <= 0.60):
            return None
        
        # Check channel parallelism (standard error of residuals)
        upper_line = np.polyfit(x, flag_highs, 1)
        lower_line = np.polyfit(x, flag_lows, 1)
        channels_parallel = abs(upper_line[0] - lower_line[0]) < 0.05
        
        if not channels_parallel:
            return None
        
        # Volume declining in flag
        volume_trend = volumes[-15:].mean() < volumes[-30:-15].mean()
        
        breakout_level = flag_highs.max() * 1.005
        target_distance = pole_high - closes[pole_start]
        
        return {
            'pattern': 'BULL_FLAG',
            'direction': 'bullish',
            'pole_gain': pole_gain,
            'flag_angle': angle_deg,
            'retracement': retracement,
            'breakout_level': breakout_level,
            'target': breakout_level + target_distance,
            'confidence': 85 if volume_trend else 70
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BEAR FLAG (Bearish Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bear_flag(self, highs, lows, closes, volumes) -> dict:
        """Mirror of bull flag - sharp drop + upward drift consolidation"""
        pole_start = -30
        pole_end = -15
        pole_drop = (closes[pole_start] - closes[pole_end]) / closes[pole_start]
        
        if pole_drop < 0.10:
            return None
        
        flag_prices = closes[-15:]
        flag_highs = highs[-15:]
        flag_lows = lows[-15:]
        
        x = np.arange(len(flag_prices))
        slope, _ = np.polyfit(x, flag_prices, 1)
        angle_deg = np.degrees(np.arctan(slope / closes[-15]))
        is_upward = 10 <= angle_deg <= 45
        
        if not is_upward:
            return None
        
        pole_low = closes[pole_end]
        flag_high = flag_highs.max()
        retracement = (flag_high - pole_low) / (closes[pole_start] - closes[pole_end])
        
        if not (0.20 <= retracement <= 0.60):
            return None
        
        breakout_level = flag_lows.min() * 0.995
        target_distance = closes[pole_start] - pole_low
        
        return {
            'pattern': 'BEAR_FLAG',
            'direction': 'bearish',
            'pole_drop': pole_drop,
            'flag_angle': angle_deg,
            'breakout_level': breakout_level,
            'target': breakout_level - target_distance,
            'confidence': 85
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BULL PENNANT (Bullish Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_bull_pennant(self, highs, lows, closes) -> dict:
        """
        Similar to bull flag BUT:
        - Consolidation forms converging triangle (not parallel channel)
        - Symmetrical contraction
        - Typically shorter duration (5-10 bars vs 10-20)
        """
        # Detect pole
        pole_gain = (closes[-12] - closes[-25]) / closes[-25]
        if pole_gain < 0.08:
            return None
        
        # Detect pennant (converging trendlines)
        pennant_highs = highs[-12:]
        pennant_lows = lows[-12:]
        
        # Fit trendlines
        x = np.arange(len(pennant_highs))
        upper_slope = np.polyfit(x, pennant_highs, 1)[0]
        lower_slope = np.polyfit(x, pennant_lows, 1)[0]
        
        # Should converge (upper falling, lower rising)
        is_converging = upper_slope < 0 and lower_slope > 0
        
        if not is_converging:
            return None
        
        # Check contraction velocity
        initial_range = pennant_highs[0] - pennant_lows[0]
        current_range = pennant_highs[-1] - pennant_lows[-1]
        contraction = (initial_range - current_range) / initial_range
        
        if contraction < 0.50:  # Should contract â‰¥50%
            return None
        
        breakout_level = pennant_highs.max() * 1.005
        target_distance = closes[-12] - closes[-25]
        
        return {
            'pattern': 'BULL_PENNANT',
            'direction': 'bullish',
            'pole_gain': pole_gain,
            'contraction': contraction,
            'breakout_level': breakout_level,
            'target': breakout_level + target_distance,
            'confidence': 80
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RISING WEDGE (Bearish Reversal - False Continuation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_rising_wedge(self, highs, lows, closes, volumes) -> dict:
        """
        Deceptive pattern - looks bullish but breaks down
        - Both trendlines rising
        - But converging (slope[low] > slope[high])
        - Volume declining (exhaustion)
        - Usually breaks DOWN despite upward trajectory
        """
        lookback = 30
        swing_highs = highs[-lookback:]
        swing_lows = lows[-lookback:]
        
        x = np.arange(lookback)
        upper_slope = np.polyfit(x, swing_highs, 1)[0]
        lower_slope = np.polyfit(x, swing_lows, 1)[0]
        
        # Both rising, but lower faster (converging)
        is_rising = upper_slope > 0 and lower_slope > 0
        is_converging = lower_slope > upper_slope
        
        if not (is_rising and is_converging):
            return None
        
        # Volume declining (key signal)
        vol_declining = volumes[-10:].mean() < volumes[-30:-10].mean()
        
        if not vol_declining:
            return None
        
        # Usually breaks down through lower trendline
        lower_trendline = lower_slope * lookback + swing_lows[0]
        breakout_level = lower_trendline * 0.995
        
        return {
            'pattern': 'RISING_WEDGE',
            'direction': 'bearish',  # Counter-intuitive!
            'note': 'False continuation - expect downward break',
            'breakout_level': breakout_level,
            'confidence': 70,
            'volume_declining': vol_declining
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FALLING WEDGE (Bullish Reversal)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_falling_wedge(self, highs, lows) -> dict:
        """Mirror of rising wedge - both falling but converging, breaks UP"""
        lookback = 30
        swing_highs = highs[-lookback:]
        swing_lows = lows[-lookback:]
        
        x = np.arange(lookback)
        upper_slope = np.polyfit(x, swing_highs, 1)[0]
        lower_slope = np.polyfit(x, swing_lows, 1)[0]
        
        is_falling = upper_slope < 0 and lower_slope < 0
        is_converging = abs(upper_slope) > abs(lower_slope)
        
        if not (is_falling and is_converging):
            return None
        
        upper_trendline = upper_slope * lookback + swing_highs[0]
        breakout_level = upper_trendline * 1.005
        
        return {
            'pattern': 'FALLING_WEDGE',
            'direction': 'bullish',
            'note': 'Expect upward breakout',
            'breakout_level': breakout_level,
            'confidence': 70
        }
```

---

## 3. CONFIDENCE SCORING SYSTEM

```python
class PatternConfidenceScorer:
    """
    Normalize confidence across all pattern types
    Score range: 0-100
    """
    
    def calculate_confidence(self, pattern_data: dict, market_context: dict) -> float:
        """
        Weighted scoring:
        - Geometric accuracy (30%)
        - Volume confirmation (25%)
        - Trend alignment (20%)
        - Breakout strength (15%)
        - Time compression (10%)
        """
        
        # 1. GEOMETRIC ACCURACY (30 points)
        geometry_score = self._score_geometry(pattern_data)
        
        # 2. VOLUME CONFIRMATION (25 points)
        volume_score = self._score_volume(pattern_data, market_context)
        
        # 3. TREND ALIGNMENT (20 points)
        trend_score = self._score_trend_alignment(pattern_data, market_context)
        
        # 4. BREAKOUT STRENGTH (15 points)
        breakout_score = self._score_breakout_strength(pattern_data, market_context)
        
        # 5. TIME COMPRESSION (10 points)
        time_score = self._score_time_validity(pattern_data)
        
        total = geometry_score + volume_score + trend_score + breakout_score + time_score
        
        return min(100, max(0, total))
    
    def _score_geometry(self, pattern: dict) -> float:
        """
        Pattern-specific geometric validation
        """
        pattern_type = pattern['pattern']
        
        if 'TRIANGLE' in pattern_type:
            # Check convergence quality
            slope_diff = abs(pattern.get('resistance_slope', 0) - pattern.get('support_slope', 0))
            # Better convergence = higher score
            return 30 * (1 - min(1, slope_diff / 0.5))
        
        elif 'HEAD_AND_SHOULDERS' in pattern_type:
            # Check symmetry
            symmetry = pattern.get('symmetry_score', 0.5)
            return 30 * symmetry
        
        elif 'FLAG' in pattern_type:
            # Check retracement depth (ideal: 38.2% Fibonacci)
            retracement = pattern.get('retracement', 0.5)
            deviation = abs(retracement - 0.382)
            return 30 * (1 - min(1, deviation / 0.2))
        
        elif 'CUP_AND_HANDLE' in pattern_type:
            # Check U-shape RÂ²
            r_squared = pattern.get('u_shape_r2', 0)
            return 30 * r_squared
        
        else:
            # Candlestick patterns - check body/shadow ratios
            return 25  # Base score for meeting criteria
    
    def _score_volume(self, pattern: dict, context: dict) -> float:
        """
        Volume confirmation:
        - Declining into pattern formation (good)
        - Surge on breakout (critical)
        """
        vol_ratio = context.get('volume_ratio', 1.0)  # Current vol / 20MA
        
        if vol_ratio > 2.0:
            return 25  # Strong surge
        elif vol_ratio > 1.5:
            return 20
        elif vol_ratio > 1.2:
            return 15
        else:
            return 5  # Weak volume = weak signal
    
    def _score_trend_alignment(self, pattern: dict, context: dict) -> float:
        """
        Pattern direction matches prevailing trend
        """
        pattern_direction = pattern['direction']
        trend_slope = context.get('trend_slope', 0)  # From MA50
        
        if pattern_direction == 'bullish' and trend_slope > 0:
            return 20  # Aligned
        elif pattern_direction == 'bearish' and trend_slope < 0:
            return 20  # Aligned
        elif abs(trend_slope) < 0.01:
            return 10  # Neutral trend
        else:
            return 5  # Counter-trend (risky)
    
    def _score_breakout_strength(self, pattern: dict, context: dict) -> float:
        """
        How decisively price broke through key level
        """
        breakout_distance = context.get('breakout_distance_pct', 0)  # % beyond level
        
        if breakout_distance > 2.0:
            return 15  # Strong breakout (>2%)
        elif breakout_distance > 1.0:
            return 12
        elif breakout_distance > 0.5:
            return 8
        else:
            return 3  # Barely broke out
    
    def _score_time_validity(self, pattern: dict) -> float:
        """
        Pattern formation timeframe appropriate
        """
        bars_ago = pattern.get('bars_since_formation', 0)
        
        # Fresh patterns score higher
        if bars_ago < 3:
            return 10  # Just formed
        elif bars_ago < 10:
            return 7
        elif bars_ago < 20:
            return 4
        else:
            return 1  # Stale pattern

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AGGREGATE SCORING FOR MULTIPLE PATTERNS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def aggregate_pattern_confidence(patterns: list) -> dict:
    """
    When multiple patterns detected:
    - Take weighted average favoring stronger patterns
    - Boost score if patterns confirm each other
    """
    if not patterns:
        return {'confidence': 0, 'strongest': None}
    
    # Sort by confidence
    sorted_patterns = sorted(patterns, key=lambda x: x['confidence'], reverse=True)
    strongest = sorted_patterns[0]
    
    # Check for confluence (multiple patterns same direction)
    same_direction = [p for p in patterns if p['direction'] == strongest['direction']]
    
    if len(same_direction) >= 2:
        # Boost confidence by 10% for confirmation
        boosted_confidence = min(100, strongest['confidence'] * 1.1)
    else:
        boosted_confidence = strongest['confidence']
    
    return {
        'confidence': round(boosted_confidence, 1),
        'strongest': strongest['pattern'],
        'total_patterns': len(patterns),
        'confluence': len(same_direction) >= 2
    }
```

---

## 4. AI VALIDATION PROMPT TEMPLATE

```python
AI_VALIDATION_PROMPT = """You are an expert quantitative trader validating technical breakout signals.

### STOCK: {symbol}
### DIRECTION: {direction}

### 6-CRITERIA SCORES:
1. Momentum Score: {momentum_score}/1.0
2. Volume Ratio: {volume_ratio}x average
3. Trend Strength: {trend_strength}/1.0
4. Volatility Percentile: {volatility_percentile}th
5. Support/Resistance Distance: {sr_distance}%
6. Price Action Score: {price_action_score}/1.0

### DETECTED CHART PATTERNS:
{patterns_list}

### PRICE CONTEXT:
- Current Price: â‚¹{current_price}
- Today's Change: {day_change_pct}%
- Week High: â‚¹{week_high}
- Volume Surge: {volume_surge}x
- Breakout Level: â‚¹{breakout_level}
- Pattern Confidence: {pattern_confidence}/100

---

## YOUR TASK:
Classify this breakout signal into ONE of these categories:

1. **TRUE_POSITIVE** - High-probability valid breakout
   - All criteria strongly aligned
   - Pattern(s) clearly confirmed
   - Volume supports move
   - Recommend entry

2. **FALSE_POSITIVE** - Likely noise or trap
   - Criteria met but context weak
   - Pattern incomplete or low-quality
   - Volume suspicious
   - Advise skip

3. **WEAK_UNCONFIRMED** - Borderline case
   - Some criteria strong, others weak
   - Pattern present but not ideal
   - Needs more confirmation
   - Suggest wait-and-watch

## OUTPUT FORMAT (JSON):
{{
  "verdict": "TRUE_POSITIVE" | "FALSE_POSITIVE" | "WEAK_UNCONFIRMED",
  "confidence": 0.XX,
  "reasoning": "2-3 sentence explanation focusing on KEY factors",
  "risk_factors": ["Factor 1", "Factor 2"],
  "entry_suggestion": "Specific actionable advice"
}}

Be concise. Focus on DISQUALIFYING factors (what could go wrong).
Traders value skepticism over hype.
"""

def build_ai_validation_request(signal: dict, criteria: dict, patterns: list) -> dict:
    """
    Prepare payload for LLM API call
    """
    # Format patterns list
    patterns_text = "\n".join([
        f"- {p['pattern']}: {p['confidence']}% confidence, "
        f"breakout @ â‚¹{p['breakout_level']:.2f}, target â‚¹{p['target']:.2f}"
        for p in patterns
    ])
    
    prompt = AI_VALIDATION_PROMPT.format(
        symbol=signal['symbol'],
        direction=signal['direction'],
        momentum_score=criteria['momentum_score'],
        volume_ratio=criteria['volume_ratio'],
        trend_strength=criteria['trend_strength'],
        volatility_percentile=criteria['volatility_percentile'],
        sr_distance=criteria['support_resistance_distance'],
        price_action_score=criteria['price_action_score'],
        patterns_list=patterns_text,
        current_price=signal['current_price'],
        day_change_pct=signal['day_change_pct'],
        week_high=signal['week_high'],
        volume_surge=signal['volume_surge'],
        breakout_level=patterns[0]['breakout_level'],
        pattern_confidence=patterns[0]['confidence']
    )
    
    return {
        "model": "llama-3.3-70b-versatile",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.1,
        "max_tokens": 250,
        "response_format": {"type": "json_object"}
    }
```

---

## 5. DASHBOARD OUTPUT SCHEMA

```typescript
interface BreakoutSignal {
  // Core Identity
  symbol: string;
  direction: 'bullish' | 'bearish';
  timestamp: Date;
  
  // Price Data
  current_price: number;
  day_change_pct: number;
  volume_ratio: number;
  
  // 6-Criteria Pass/Fail
  criteria: {
    momentum: { passed: boolean; score: number };
    volume: { passed: boolean; score: number };
    trend: { passed: boolean; score: number };
    volatility: { passed: boolean; score: number };
    support_resistance: { passed: boolean; score: number };
    price_action: { passed: boolean; score: number };
  };
  
  // Pattern Analysis (NON-AI)
  patterns: {
    detected: PatternMatch[];  // All matched patterns
    strongest: {
      name: string;
      confidence: number;
      breakout_level: number;
      target: number;
      type: 'candlestick' | 'triangle' | 'reversal' | 'continuation';
    };
    aggregate_confidence: number;  // Combined score 0-100
    confluence: boolean;  // Multiple patterns agree?
  };
  
  // AI Validation (LAZY LOADED - null until clicked)
  ai_validation: {
    status: null | 'pending' | 'completed' | 'error';
    verdict: null | 'TRUE_POSITIVE' | 'FALSE_POSITIVE' | 'WEAK_UNCONFIRMED';
    confidence: null | number;
    reasoning: null | string;
    risk_factors: null | string[];
    entry_suggestion: null | string;
    validated_at: null | Date;
  };
}

interface PatternMatch {
  pattern: string;
  direction: 'bullish' | 'bearish';
  confidence: number;
  breakout_level: number;
  target: number;
  geometry_score: number;
  volume_confirmed: boolean;
  bars_since_formation: number;
  metadata: Record<string, any>;  // Pattern-specific details
}
```

### Database Table Schema (PostgreSQL)

```sql
-- Main signals table (existing, add pattern columns)
ALTER TABLE bullish_breakout_nse_eq ADD COLUMN IF NOT EXISTS
  detected_patterns JSONB,
  strongest_pattern VARCHAR(50),
  pattern_confidence NUMERIC(5,2),
  ai_validation_status VARCHAR(20),
  ai_verdict VARCHAR(20),
  ai_confidence NUMERIC(4,3),
  ai_reasoning TEXT,
  ai_validated_at TIMESTAMP;

-- Index for pattern filtering
CREATE INDEX idx_pattern_confidence ON bullish_breakout_nse_eq(pattern_confidence DESC);
CREATE INDEX idx_strongest_pattern ON bullish_breakout_nse_eq(strongest_pattern);
CREATE INDEX idx_ai_verdict ON bullish_breakout_nse_eq(ai_verdict);

-- View for UI consumption
CREATE OR REPLACE VIEW screener_signals_with_patterns AS
SELECT 
  symbol,
  direction,
  current_price,
  day_change_pct,
  volume_ratio,
  
  -- Criteria summary
  jsonb_build_object(
    'momentum', momentum_score,
    'volume', volume_ratio,
    'trend', trend_strength,
    'volatility', volatility_percentile,
    'sr_distance', support_resistance_distance,
    'price_action', price_action_score
  ) as criteria_scores,
  
  -- Pattern info (always loaded)
  detected_patterns,
  strongest_pattern,
  pattern_confidence,
  
  -- AI validation (lazy loaded)
  CASE 
    WHEN ai_validation_status IS NULL THEN NULL
    ELSE jsonb_build_object(
      'status', ai_validation_status,
      'verdict', ai_verdict,
      'confidence', ai_confidence,
      'reasoning', ai_reasoning,
      'validated_at', ai_validated_at
    )
  END as ai_validation,
  
  created_at,
  updated_at
FROM bullish_breakout_nse_eq
WHERE created_at > NOW() - INTERVAL '15 minutes'
ORDER BY pattern_confidence DESC, volume_ratio DESC;
```

---

## 6. LOW-LATENCY OPTIMIZATION STRATEGY

### 6.1 Pre-Computation Pipeline

```python
class OptimizedPatternScanner:
    """
    Main scanner with aggressive caching and vectorization
    """
    
    def __init__(self, universe_size=2500):
        self.universe_size = universe_size
        
        # Pre-allocate arrays (avoid repeated allocation)
        self.ohlcv_buffer = np.zeros((universe_size, 100, 5))  # 100-bar window
        self.indicators_cache = {}
        self.pattern_results_cache = {}
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 1: BULK DATA LOAD (PostgreSQL â†’ NumPy)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def load_market_data_vectorized(self, symbols: list) -> np.ndarray:
        """
        Single bulk query â†’ NumPy array
        Avoid per-symbol queries
        """
        # Use psycopg2 with COPY for maximum speed
        conn = psycopg2.connect(...)
        
        query = """
        COPY (
          SELECT symbol, timestamp, open, high, low, close, volume
          FROM historical_prices
          WHERE symbol = ANY(%s)
            AND timestamp >= NOW() - INTERVAL '100 bars'
          ORDER BY symbol, timestamp
        ) TO STDOUT WITH CSV
        """
        
        # Stream directly to NumPy
        cursor = conn.cursor()
        cursor.copy_expert(query, buffer)
        data = np.loadtxt(buffer, delimiter=',')
        
        # Reshape to [N_stocks x N_bars x 5_OHLCV]
        reshaped = data.reshape(len(symbols), -1, 7)[:, :, 2:]  # Drop symbol, timestamp
        
        return reshaped
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 2: INDICATOR PRE-COMPUTATION (Cached)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def precompute_indicators(self, ohlcv: np.ndarray):
        """
        Vectorized indicator calculation for all stocks at once
        Cache results for 5 minutes
        """
        cache_key = f"indicators_{datetime.now().strftime('%H%M')}"  # 5-min buckets
        
        if cache_key in self.indicators_cache:
            return self.indicators_cache[cache_key]
        
        c = ohlcv[:, :, 3]  # Close prices
        h = ohlcv[:, :, 1]  # Highs
        l = ohlcv[:, :, 2]  # Lows
        v = ohlcv[:, :, 4]  # Volume
        
        indicators = {
            # Rolling windows (vectorized across all stocks)
            'high_5': bottleneck.move_max(h, window=5, axis=1),
            'high_10': bottleneck.move_max(h, window=10, axis=1),
            'high_20': bottleneck.move_max(h, window=20, axis=1),
            'high_50': bottleneck.move_max(h, window=50, axis=1),
            'low_5': bottleneck.move_min(l, window=5, axis=1),
            'low_10': bottleneck.move_min(l, window=10, axis=1),
            'low_20': bottleneck.move_min(l, window=20, axis=1),
            'low_50': bottleneck.move_min(l, window=50, axis=1),
            
            # ATR (vectorized True Range calculation)
            'atr_14': self._vectorized_atr(h, l, c, period=14),
            
            # Volume MA
            'volume_ma20': bottleneck.move_mean(v, window=20, axis=1),
            
            # Price MAs
            'ma_20': bottleneck.move_mean(c, window=20, axis=1),
            'ma_50': bottleneck.move_mean(c, window=50, axis=1),
            
            # Swing points (local maxima/minima)
            'pivots_high': self._detect_pivots_vectorized(h, window=5),
            'pivots_low': self._detect_pivots_vectorized(l, window=5),
        }
        
        self.indicators_cache[cache_key] = indicators
        return indicators
    
    def _vectorized_atr(self, h, l, c, period=14):
        """ATR calculation without loops"""
        tr = np.maximum(
            h - l,
            np.maximum(
                np.abs(h - np.roll(c, 1, axis=1)),
                np.abs(l - np.roll(c, 1, axis=1))
            )
        )
        return bottleneck.move_mean(tr, window=period, axis=1)
    
    def _detect_pivots_vectorized(self, prices, window=5):
        """
        Sliding window max/min detection
        Using scipy.ndimage.maximum_filter (super fast)
        """
        from scipy.ndimage import maximum_filter, minimum_filter
        
        # Local maxima = point equals max in surrounding window
        local_max = maximum_filter(prices, size=(1, window), mode='constant')
        pivots = (prices == local_max)
        
        return pivots
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 3: PARALLEL PATTERN DETECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def detect_all_patterns_parallel(self, ohlcv, indicators):
        """
        Run pattern detection in parallel using multiprocessing
        """
        from multiprocessing import Pool
        import numba
        
        # Split stocks into chunks for parallel processing
        n_workers = 8  # CPU cores
        chunk_size = self.universe_size // n_workers
        
        chunks = [
            (ohlcv[i:i+chunk_size], indicators)
            for i in range(0, self.universe_size, chunk_size)
        ]
        
        with Pool(n_workers) as pool:
            results = pool.starmap(self._detect_patterns_chunk, chunks)
        
        # Flatten results
        return [item for sublist in results for item in sublist]
    
    @staticmethod
    @numba.jit(nopython=True, parallel=True)
    def _detect_patterns_chunk(ohlcv_chunk, indicators):
        """
        Numba-accelerated pattern detection
        Runs at near-C speed
        """
        results = []
        
        for i in numba.prange(len(ohlcv_chunk)):
            # Run all pattern detectors
            candlestick_patterns = detect_candlestick_patterns_numba(ohlcv_chunk[i])
            triangle_patterns = detect_triangles_numba(ohlcv_chunk[i])
            # ... etc
            
            results.append({
                'stock_idx': i,
                'patterns': candlestick_patterns + triangle_patterns
            })
        
        return results
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 4: BATCH DATABASE WRITE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def bulk_insert_signals(self, signals: list):
        """
        Single bulk INSERT instead of N individual writes
        Use PostgreSQL COPY or execute_values()
        """
        from psycopg2.extras import execute_values
        
        conn = psycopg2.connect(...)
        cursor = conn.cursor()
        
        # Prepare data tuples
        data = [
            (
                s['symbol'],
                s['direction'],
                s['current_price'],
                json.dumps(s['patterns']),
                s['strongest_pattern'],
                s['pattern_confidence'],
                datetime.now()
            )
            for s in signals
        ]
        
        # Bulk insert (50x faster than executemany)
        execute_values(
            cursor,
            """
            INSERT INTO bullish_breakout_nse_eq 
            (symbol, direction, current_price, detected_patterns, 
             strongest_pattern, pattern_confidence, created_at)
            VALUES %s
            ON CONFLICT (symbol, created_at) DO UPDATE SET
              detected_patterns = EXCLUDED.detected_patterns,
              pattern_confidence = EXCLUDED.pattern_confidence
            """,
            data,
            page_size=500
        )
        
        conn.commit()
```

### 6.2 Performance Optimization Checklist

```markdown
## âœ… DATA LOADING
- [x] Single bulk query (not per-symbol)
- [x] Use PostgreSQL COPY for large reads
- [x] Stream data directly to NumPy arrays
- [x] Connection pooling (reuse DB connections)

## âœ… COMPUTATION
- [x] Pre-compute indicators once per scan
- [x] Vectorize all array operations (no loops)
- [x] Use bottleneck for moving window ops (20x faster than pandas)
- [x] Numba JIT compilation for hot paths
- [x] Parallel processing (multiprocessing Pool)
- [x] Avoid Python loops over stocks - batch process

## âœ… CACHING STRATEGY
- [x] In-memory cache for indicators (5-min TTL)
- [x] Redis cache for symbol metadata (24-hour TTL)
- [x] Pattern results cached per scan cycle
- [x] Avoid recomputing unchanged data

## âœ… DATABASE WRITES
- [x] Batch inserts (execute_values)
- [x] Upsert pattern (ON CONFLICT DO UPDATE)
- [x] Index on pattern_confidence for fast sorting
- [x] Partial index on ai_validation_status for filtering

## âœ… FRONTEND OPTIMIZATION
- [x] API returns paginated results (50 per page)
- [x] Pattern data included in initial response
- [x] AI validation lazy-loaded (separate endpoint)
- [x] WebSocket for real-time updates (optional)
- [x] Client-side caching (React Query with 60s stale time)

## âœ… PROFILING & MONITORING
- [x] Log scan duration per stage
- [x] Track cache hit rates
- [x] Monitor database query times
- [x] Alert if scan exceeds 200ms budget
```

### 6.3 Expected Performance Metrics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE                    â”‚ LATENCY (2500 stocks)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage 1: Rule Filter     â”‚  20ms  (existing)              â”‚
â”‚ Stage 2a: Data Load      â”‚  30ms  (bulk query)            â”‚
â”‚ Stage 2b: Indicators     â”‚  15ms  (cached 90% of time)    â”‚
â”‚ Stage 2c: Patterns       â”‚  50ms  (parallel + Numba)      â”‚
â”‚ Stage 2d: Confidence     â”‚  10ms  (vectorized)            â”‚
â”‚ Stage 2e: DB Write       â”‚  25ms  (bulk insert)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL (Stage 1+2)        â”‚ 150ms  âœ… Under 200ms budget   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage 3: AI Validation   â”‚ 500ms  (per stock, on-demand)  â”‚
â”‚                          â”‚ User-triggered, async          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Memory Usage:
- OHLCV buffer: 2500 stocks Ã— 100 bars Ã— 5 Ã— 8 bytes = 10 MB
- Indicators cache: ~30 MB
- Pattern results: ~5 MB
TOTAL: <50 MB RAM per scan (negligible)

Throughput:
- 2500 stocks / 150ms = 16,666 stocks/second
- Easily scales to 10K universe if needed
```

---

## 7. IMPLEMENTATION ROADMAP

### Phase 1: Core Pattern Engine (Week 1)
- [ ] Implement candlestick detector (11 patterns)
- [ ] Implement triangle detector (3 patterns)
- [ ] Add confidence scoring
- [ ] Unit tests for each pattern
- [ ] Benchmark on historical data

### Phase 2: Advanced Patterns (Week 2)
- [ ] Reversal patterns (7 patterns)
- [ ] Continuation patterns (7 patterns)
- [ ] Optimize pivot detection
- [ ] Add pattern confluence logic

### Phase 3: Integration (Week 3)
- [ ] Integrate into existing scanner
- [ ] Add pattern columns to database
- [ ] Update API endpoints
- [ ] Frontend UI for pattern display

### Phase 4: AI Validation (Week 4)
- [ ] Groq API integration
- [ ] "AI Validate" button in UI
- [ ] Prompt engineering and testing
- [ ] Rate limiting and error handling

### Phase 5: Optimization (Week 5)
- [ ] Numba compilation of hot paths
- [ ] Parallel processing implementation
- [ ] Caching layer refinement
- [ ] Performance profiling and tuning

---

## 8. RISK MITIGATION

### False Positives
- **Problem:** Patterns detected in noise
- **Solution:** Minimum confidence threshold (70+)
- **Solution:** Volume confirmation mandatory
- **Solution:** AI validation acts as final filter

### Performance Degradation
- **Problem:** Scanner exceeds 200ms budget
- **Solution:** Adaptive pattern selection (skip complex patterns if time low)
- **Solution:** Circuit breaker - fail gracefully if DB slow
- **Solution:** Monitoring alerts on p95 latency

### AI Validation Failures
- **Problem:** Groq API down or rate-limited
- **Solution:** Fallback to pattern confidence score
- **Solution:** Queue validation requests (async processing)
- **Solution:** Cache validation results (don't re-validate same signal)

### Over-Reliance on AI
- **Problem:** Traders blindly trust AI verdict
- **Solution:** Show AI as "assistant" not "decision maker"
- **Solution:** Display confidence scores prominently
- **Solution:** Encourage manual chart review

---

## 9. GROQ API INTEGRATION CODE

```typescript
// src/app/api/validate-breakout/route.ts

import { NextRequest, NextResponse } from 'next/server';
import Groq from 'groq-sdk';

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY,
});

export async function POST(request: NextRequest) {
  try {
    const { symbol, direction, criteria, patterns, price_context } = await request.json();
    
    // Rate limiting check (max 100/day per user)
    // ... implement rate limit logic
    
    // Build prompt
    const prompt = buildValidationPrompt(symbol, direction, criteria, patterns, price_context);
    
    // Call Groq API
    const completion = await groq.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      model: 'llama-3.3-70b-versatile',
      temperature: 0.1,
      max_tokens: 250,
      response_format: { type: 'json_object' },
    });
    
    const result = JSON.parse(completion.choices[0].message.content);
    
    // Update database with validation result
    await supabase
      .from('bullish_breakout_nse_eq')
      .update({
        ai_validation_status: 'completed',
        ai_verdict: result.verdict,
        ai_confidence: result.confidence,
        ai_reasoning: result.reasoning,
        ai_validated_at: new Date().toISOString(),
      })
      .eq('symbol', symbol)
      .eq('direction', direction)
      .gte('created_at', new Date(Date.now() - 15 * 60 * 1000).toISOString());
    
    return NextResponse.json({
      success: true,
      validation: result,
    });
    
  } catch (error) {
    console.error('AI validation error:', error);
    return NextResponse.json(
      { success: false, error: 'Validation failed' },
      { status: 500 }
    );
  }
}

function buildValidationPrompt(symbol, direction, criteria, patterns, context) {
  const patterns_text = patterns.map(p => 
    `- ${p.pattern}: ${p.confidence}% confidence, ` +
    `breakout @ â‚¹${p.breakout_level.toFixed(2)}, ` +
    `target â‚¹${p.target.toFixed(2)}`
  ).join('\n');
  
  return `You are an expert quantitative trader validating technical breakout signals.

### STOCK: ${symbol}
### DIRECTION: ${direction}

### 6-CRITERIA SCORES:
1. Momentum Score: ${criteria.momentum_score}/1.0
2. Volume Ratio: ${criteria.volume_ratio}x average
3. Trend Strength: ${criteria.trend_strength}/1.0
4. Volatility Percentile: ${criteria.volatility_percentile}th
5. Support/Resistance Distance: ${criteria.sr_distance}%
6. Price Action Score: ${criteria.price_action_score}/1.0

### DETECTED CHART PATTERNS:
${patterns_text}

### PRICE CONTEXT:
- Current Price: â‚¹${context.current_price}
- Today's Change: ${context.day_change_pct}%
- Week High: â‚¹${context.week_high}
- Volume Surge: ${context.volume_surge}x
- Breakout Level: â‚¹${context.breakout_level}

---

Classify this breakout signal into ONE of these categories:

1. **TRUE_POSITIVE** - High-probability valid breakout
2. **FALSE_POSITIVE** - Likely noise or trap
3. **WEAK_UNCONFIRMED** - Borderline case

OUTPUT FORMAT (JSON):
{
  "verdict": "TRUE_POSITIVE" | "FALSE_POSITIVE" | "WEAK_UNCONFIRMED",
  "confidence": 0.XX,
  "reasoning": "2-3 sentence explanation",
  "risk_factors": ["Factor 1", "Factor 2"],
  "entry_suggestion": "Specific actionable advice"
}

Be concise. Focus on DISQUALIFYING factors.`;
}
```

---

## 10. FRONTEND UI COMPONENT

```tsx
// components/screener/PatternBadge.tsx

import { Badge } from "@/components/ui/badge";
import { Button } from "@/components/ui/button";
import { Sparkles, AlertTriangle, CheckCircle } from "lucide-react";
import { useState } from "react";

interface PatternBadgeProps {
  signal: BreakoutSignal;
  onValidate: (symbol: string) => Promise<void>;
}

export function PatternBadge({ signal, onValidate }: PatternBadgeProps) {
  const [isValidating, setIsValidating] = useState(false);
  
  const handleValidate = async () => {
    setIsValidating(true);
    await onValidate(signal.symbol);
    setIsValidating(false);
  };
  
  const getConfidenceColor = (conf: number) => {
    if (conf >= 80) return "bg-green-500";
    if (conf >= 60) return "bg-yellow-500";
    return "bg-orange-500";
  };
  
  return (
    <div className="space-y-2">
      {/* Pattern Info */}
      <div className="flex items-center gap-2">
        <Badge variant="outline" className={getConfidenceColor(signal.patterns.aggregate_confidence)}>
          {signal.patterns.strongest.name}
        </Badge>
        <span className="text-xs text-muted-foreground">
          {signal.patterns.aggregate_confidence}% confidence
        </span>
        {signal.patterns.confluence && (
          <Badge variant="secondary" className="text-xs">
            Multiple Patterns âœ“
          </Badge>
        )}
      </div>
      
      {/* AI Validation */}
      {!signal.ai_validation.status && (
        <Button
          size="sm"
          variant="outline"
          onClick={handleValidate}
          disabled={isValidating}
          className="w-full"
        >
          <Sparkles className="h-4 w-4 mr-2" />
          {isValidating ? "Validating..." : "AI Validate"}
        </Button>
      )}
      
      {signal.ai_validation.status === 'completed' && (
        <div className={`p-2 rounded border ${
          signal.ai_validation.verdict === 'TRUE_POSITIVE' ? 'border-green-500 bg-green-50' :
          signal.ai_validation.verdict === 'FALSE_POSITIVE' ? 'border-red-500 bg-red-50' :
          'border-yellow-500 bg-yellow-50'
        }`}>
          <div className="flex items-center gap-2 mb-1">
            {signal.ai_validation.verdict === 'TRUE_POSITIVE' && <CheckCircle className="h-4 w-4 text-green-600" />}
            {signal.ai_validation.verdict === 'FALSE_POSITIVE' && <AlertTriangle className="h-4 w-4 text-red-600" />}
            <span className="text-sm font-semibold">
              {signal.ai_validation.verdict.replace('_', ' ')}
            </span>
            <span className="text-xs text-muted-foreground">
              ({Math.round(signal.ai_validation.confidence * 100)}%)
            </span>
          </div>
          <p className="text-xs text-gray-700">
            {signal.ai_validation.reasoning}
          </p>
        </div>
      )}
    </div>
  );
}
```

---

**END OF TECHNICAL SPECIFICATION**

This system provides deterministic, rule-based pattern detection with AI validation as a final quality check, ensuring sub-200ms performance at scale.
